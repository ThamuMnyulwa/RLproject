{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "## Return of the Multi-Armed Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-armed bandit problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refered to as \"Multi arm bandit problem\" because when you pull on a slot machine you use your arm and they are reffered to as bandits because they are takong your money.\n",
    "\n",
    "- Go to a casino and choose between 3 slot machines\n",
    "- Each slot machine can give you a value (reward) between 0 (lose) and 1 (win) \n",
    "- Unknown win rate of a slot machine e.g 0.2,0.4, e.c.t.\n",
    "- Goal: maximize your reward (win rate)\n",
    "- Problem: Can only discover the best machine (bandit) by collecting the data\n",
    "- If you could, you would guess the slot machine (bandit) with the highest win rate (you don't know the true win rate but you need to estimate the win rate of each machine by collecting the data).\n",
    "-  You find a balance , explore (collecting data) + exploit (playing \"best-so-far\" machine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dilema comes in that you need to collect lots of data to get correct estimates but if you spend a lot of time collecting playing suboptimal bandits. Hence you need to balance the ammount of time trying to maximise your earnings and trying to obtain accurate measurements of each bandits win rate, since in order to maximise your earnings you need to accurately know that information.\n",
    "\n",
    "https://link.springer.com/article/10.1023%2FA%3A1013689704352"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional A/B testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A/B testing is a randomized experiment with 2 variants. It includes application in two-sample hypothesis testing. A/B testing is a way to compare two versions of a single variable, typically by testing a subject's response to variant A against variant B, and determining which of the two variants is more effective.\n",
    "\n",
    "In traditional A/B testing you would predetermine ahead of time the number if times you need to play each slot machine in order to obtain statistical significance. i.e use bayesian learning with a predetermined prior. The number of times one needs to play is dependent on different factors such as the difference in win rates between each bandit. If the difference is bigger you need less samples , similarly if the difference is smaller you need more samples ( there is an inverse relationship between samples needed and the difference in win rate between samples). However unfortunately you do not know the difference between each test before you commence testing. This is a major disadvantage of traditional A/B testing.\n",
    "\n",
    "It is also important to note that statistical significance will only be achieved once a certain number of tests have occured in what refer to here as traditional A/B testing i.e we need a large enough sample size.\n",
    "\n",
    "https://en.wikipedia.org/wiki/A/B_testing\n",
    "\n",
    "### Behavioural Biases (Behavioural Economics Approach)\n",
    "\n",
    "This is not the same as human behaviour (emotion) that tends to carry a bias towards taking the most recent result, this is an example of what they refer to in behavioural economics as \"anchoring\". We know that \"Information-processing biases result in information being processed and used illogically or irrationally.\" Anchoring which is a form of information processing biases can be thought of as when we overly rely on recent information. \n",
    "\n",
    "Say I play 2 slot machines 3 times each and win twice on one and zero times on the other. I could beleive that the win rate on one slot machine is 0% and the win rate on the other machine is (2/3) 67% . We know from a statistical point of view this would be incorrect as our sample size is too small and we would need to perform more iterations. \n",
    "\n",
    "https://www.cfainstitute.org/en/membership/professional-development/refresher-readings/2020/behavioral-biases-individuals\n",
    "\n",
    "Perhaps what we need is algorithms that would systematically make an explore-exploit tradeoff. These would hopefully be better than the previour 2 suboptimal solutions to solve our problem and maximize the reward function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Applications of Explore-Exploit\n",
    "\n",
    "###### 2 advertisement example\n",
    "- Example\n",
    "Quantitatively comparing things applies to almost any business. \n",
    "    - You have 2 ADs\n",
    "        - Which one is better? Naturally, the one that will likely lead to more purchases of the product or service. One possible way to meausure this is to determine and measure the click-through-rate (CTR) for each advertisement.\n",
    "        - $\\#$ clicks/ $\\#$ impressions, ratio is used to measure CTR.\n",
    "     - **How do I measure the CTR?**\n",
    "         - Do an experiment. Show the first advertisement to a sample $n_{1}$ then show the second advertisement to another sample $n_{2}$. \n",
    "        2.6 2:50\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Greedy\n",
    "\n",
    "The first solution to the explore-exploit dilemma is reffered to as the Epsilon-Greedy strategy. This is the most important as it is the one we will use through out most of our examples. We use a small number, \"epsilon\", to be the probability of exploration.\n",
    "\n",
    "In each round we generate a number and if the number is less than epsilon $\\epsilon$ , we **explore** (this means we just choose an arm at random). If the random number is greater than epsilon we exploit, meaning we choose the arm that has the best maximum reward rate so far. \n",
    "\n",
    "It should be clear that through this we learn which arm is best eventually even though your initial guesses might be wrong. Since every arm always has some chance to get updated. \n",
    "\n",
    "https://towardsdatascience.com/solving-multiarmed-bandits-a-comparison-of-epsilon-greedy-and-thompson-sampling-d97167ca9a50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pull best Arm.  p =  0.9850706414547037\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Epsilon = 0.05 , i.e 5% , the algorithm will exploit the best variant 95% of the time and \n",
    "# will explore random alternatives 5% of the time.\n",
    "eps = 0.05 \n",
    "\n",
    "p = np.random.random() \n",
    "if p < eps:\n",
    "    # We explore\n",
    "    # pull random arm\n",
    "    arm = \"Pull Arm on Random.\"\n",
    "else:\n",
    "    # We exploit, meaning\n",
    "    # pull current best arm\n",
    "    arm = \"Pull best Arm.\"\n",
    "# At some point we will discover which arm is the true best, since it allows us to update every arm's estimate\n",
    "print(arm,\" p = \",p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Theoreticall in the long run, the algorithm allows us to explore each arm an infinite number of times\n",
    "- Problem is we will get to a point were the values converge  and you are still exploring whilst you do not need to explore anymore.\n",
    "- So if epsilon is 10% then you will spend 10% of the time performing a suboptimal task (choosing suboptimal arms)\n",
    "- An A/B test could be useful here, Where you could do a test at a predetermined time to check for statistical significance. Once you found statistical significance you could change $\\epsilon = 0$. When $\\epsilon = 0$ you are **effectively only doing the greedy part**. However there may be better ways to adapt in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating a Sample Mean\n",
    "- Let's assume that the Bandit Rewards are not just coin tosses, what is the best way to keep track of a reward?\n",
    "- The general method to solve this problem would be to use the arithmetic mean.\n",
    "- This works for the coin toss problem (binary problem, heads as 1 and tails as 0) as if one had $N$ tosses of a coin you would arrive at the maximum likelihood \\bar{X}= \\frac{1}{N} \\sum_{i=1}^{N} X_{i} probability of getting heads by adding up the total number of heads and dividing it up by $N$.\n",
    "\n",
    "$$ \\bar{X}= \\frac{1}{N} \\sum_{i=1}^{N} X_{i} $$\n",
    "\n",
    "- What's the problem with this equation?\n",
    "\n",
    "The problem with this equation is that it requires you to store all N elements in order to calculate the mean. Let's ask ourselves is there a way to make this calculation more effecient and the answer is yes there is. We can calculate the mean at the $N$'th sample with the mean at the $N-1$'th sample. One can then say that this allows us to calculate the mean in a more computationally efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "\\bar{X}_{N} & = \\frac{1}{N} \\sum_{i=1}^{N} X_{i} \\\\&= \\frac{1}{N} \\sum_{i=1}^{N-1} X_{i} + \\frac{1}{N} X_{i} \\\\& = \\frac{N - 1}{N}  \\bar{X}_{N-1} + \\frac{1}{N} X_{N} \\\\\\bar{X}_{N} & = \\left (  1-\\frac{1}{N} \\right ) \\bar{X}_{N-1} + \\frac{1}{N} X_{N} \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This derivation to the form of the updated version is important going further.\n",
    "\n",
    "https://www.physicsforums.com/threads/updating-the-mean-and-sd-of-a-set-efficiently.526280/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing your own Bandit Program\n",
    "\n",
    "We will explore several different methods but they are all for solving the same problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised learning VS Reinforcement Learning\n",
    "\n",
    "First Lets recap how we would do this in supervised learning (Naive Bayes, Decision Trees, NN, ect...). We will make use of an example coded in python below. The first step is loading the data, followed by intiating the model, training the model and lastly evaluating the model. The job of you as the implementer of the algorithm is to write up the fit and predict functions. This is the case of all supervised learning algorithms, the algorithm changes but the layout remains somewhat the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-2-acea0e950f11>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-acea0e950f11>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    def predict(X):\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class mymodel:\n",
    "    def fit(X, Y):\n",
    "        #The job is to train the model\n",
    "    def predict(X):\n",
    "        #The job is to predict\n",
    "\n",
    "# boilerplate\n",
    "Xtrain, Ytrain, Xtest, Ytest = get_data() # 1.Get data\n",
    "model = Mymodel() # 2. Initiate model\n",
    "model.fit(X.train , Ytrain) # 3. Train model\n",
    "model.score(Xtest, Ytest) # 4. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is not our outline as we are not at this point intrested in supervised learning but in reinfircement learning. However, that still means that there is a pattern to be followed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start we will implement some actual casiono machines. We know that a \"a slot machine\" is just an analogy for real life applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAwesomeCasinoMachine:\n",
    "    def pull:\n",
    "        # simulates drawing from a true distribution\n",
    "        # Of course, which we would not know in \"real life\"\n",
    "for t in range(max_iterations):\n",
    "    #pick a casino machine to play based on the algorithm\n",
    "    #update algorithm parameters\n",
    "    \n",
    "# plot useful info (avg reward, best machine, ect.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a pull function for your slot machine that returns a sample from the true distribution from wherever your samples are coming from, but remember this is just a simulation, in real life you would plugging it into your ad serving application. The point is that you are sampling using an application which simulates a real life action with a distribution that you do not know the actual true ditribution. Since this is a simulation you actally know this distribution, you just pretend like you do not know it.\n",
    "\n",
    "With each iteration of this loop you will update your model parameters, once this loop is done you will want to print out information that is useful to you such as the average reward over time in which the slot machine your algorithm discovered is best. You would then want to compare that to the slot machine which is actually best in order to see if your algorithm learned the right answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Different Epsilons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the project we implement epsilon greedy in code and demonstrate the effect of different values of epsilon.\n",
    "\n",
    "We start off with the class \"Bandit\", the constructor takes in one parameter M, M is the true mean. We then ser the instance variables mean and N to zero. The instance variable is our estimate of the banits mean.\n",
    "\n",
    "Next we have the pull function which simulates pulling the slot machine's (bandit's) arm. from inspection one can see that every pull will be a gaussian with unit variates.\n",
    "\n",
    "Finally we make an update with the update function. This function takes in a value 'x' which is the latest sample received from the bandit. Take note of the update equation which we have just discussed.\n",
    "\n",
    "Next, we have the run experiment function which takes the three different means for each slot machine \"m1\",\"m2\",\"m3\". It has the value 'eps' representing epsilon parsed to it so that we can do epsilon greedy. It also has \"N\" parsed to it representing the number of times we put. When the function returns we would like it to return the cumulative average after every play. We then compare these plots for different settings of epsilon. We keep the results in an array of size N called data.\n",
    "\n",
    "Notice the implementation of epsilon greedy inside the loop. This is disscused in the previous example above where we generate a random number **p** between 0 and 1. If $p < \\epsilon $ , we choose a random slot machine (bandit at random). If  $p >= \\epsilon $ we choose the bandit with the current best sample mean (\"m1\",\"m2\",\"m3\").\n",
    "\n",
    "We then pull the chosen bandit and update the bandit of choice with the reward we just got and we save X. When the loop is finished we calculate the cumulative average, followed by plotting the cumulative average along with bars showing each of the means so we can see where our cumulative averages relative to those. This is done on a log scale, so that you can see the fluctuations in earlier rounds more clearly. Lastly we print each of the estimated means for debugging purposes.\n",
    "\n",
    "In the main section we do the same experiment 3 times, each for different epsilon, $1\\%$ , $5\\%$ and $10\\%$. This is followed by plotting the cumulative averages together, both a log plot and a linear plot. This is as with a log plot all the later data is squished to the right thus making it look like we spent more time being very sub-optimal than we actually do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Premable\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Bandit:\n",
    "  def __init__(self, m):  # m is the true mean\n",
    "    self.m = m\n",
    "    self.mean = 0\n",
    "    self.N = 0\n",
    "\n",
    "  def pull(self): # simulated pulling bandits arm\n",
    "    return np.random.randn() + self.m\n",
    "\n",
    "  def update(self, x):\n",
    "    self.N += 1\n",
    "    self.mean = (1 - 1.0/self.N)*self.mean + 1.0/self.N*x  # look at the derivation above of the mean\n",
    "\n",
    "\n",
    "def run_experiment(m1, m2, m3, eps, N):\n",
    "  bandits = [Bandit(m1), Bandit(m2), Bandit(m3)]\n",
    "\n",
    "  data = np.empty(N)\n",
    "  \n",
    "  for i in range(N): # Implement epsilon greedy shown above\n",
    "    # epsilon greedy\n",
    "    p = np.random.random()\n",
    "    if p < eps:\n",
    "      j = np.random.choice(3) # Explore\n",
    "    else:\n",
    "      j = np.argmax([b.mean for b in bandits]) # Exploit\n",
    "    x = bandits[j].pull()  # Pull and update\n",
    "    bandits[j].update(x)\n",
    "\n",
    "    # Results for the plot\n",
    "    data[i] = x  # Store the results in an array called data of size N\n",
    "  cumulative_average = np.cumsum(data) / (np.arange(N) + 1)      # Calculate cumulative average\n",
    "      \n",
    "\n",
    "  # plot moving average ctr\n",
    "  plt.plot(cumulative_average) # plot cumulative average\n",
    "  plt.plot(np.ones(N)*m1) # Plot bars with each of the means so we can see where are cumulative averages relative to means\n",
    "  plt.plot(np.ones(N)*m2)\n",
    "  plt.plot(np.ones(N)*m3)\n",
    "  plt.xscale('log') # We do this on a log scale so that you can see the fluctuations in earlier rounds more clearly\n",
    "  plt.show()\n",
    "\n",
    "  for b in bandits:\n",
    "    print(b.mean)\n",
    "\n",
    "  return cumulative_average\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  c_1 = run_experiment(1.0, 2.0, 3.0, 0.1, 100000) # run_experiment(m1, m2, m3, eps, N):\n",
    "  c_05 = run_experiment(1.0, 2.0, 3.0, 0.05, 100000)\n",
    "  c_01 = run_experiment(1.0, 2.0, 3.0, 0.01, 100000)\n",
    "\n",
    "  # log scale plot\n",
    "  plt.plot(c_1, label='eps = 0.10')\n",
    "  plt.plot(c_05, label='eps = 0.05')\n",
    "  plt.plot(c_01, label='eps = 0.01')\n",
    "  plt.legend()\n",
    "  plt.xscale('log')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "  # linear plot\n",
    "  plt.plot(c_1, label='eps = 0.10')\n",
    "  plt.plot(c_05, label='eps = 0.05')\n",
    "  plt.plot(c_01, label='eps = 0.01')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
