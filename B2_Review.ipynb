{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "B2_Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing important concepts in notebooks marked A. It's all theory and no coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section Outline\n",
    "- Markov Decission Processes (MDPs)\n",
    "- **Components of a RL system:** state, enviroment, actions, rewards, returns, episodes, discounting, policies, value functions.\n",
    "- **Specific Solutions to MDPs**:\n",
    "        -Dynamic Programming\n",
    "        -Monte Carlo\n",
    "        -Temporal Difference Learning\n",
    "- **Caveat**:Specific Solutions to MDPs all look the same at a high level, you need to understand the code to get the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Approximation methods\n",
    "- Can handle when state-space is large or infinite\n",
    "- Table-based learning just helps us learn the principles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:blue'>  Next we'll look at approximation methods. These are needed when the states space is large or infinite. Table based reinforcement learning only works for the smallest of problems. It is mainly a tool for learning the principles. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The later part of these notebooks that are not used in our final project will include:\n",
    "- CNNs\n",
    "- RNNs\n",
    "- Regression, Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of MDPs\n",
    "- Review of MDPs (Markov Decision Processes)\n",
    "- Components of a RL system\n",
    "- MDP Framework: MDP is a collection of 5 things:\n",
    "    - Set of all *states*\n",
    "    - Set of all *actions*\n",
    "    - Set of all *rewards*\n",
    "    - State *transition probabilities*\n",
    "    - *Discount Factor (gamma)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###### *States*\n",
    "- *State* represents what the sensors of your agent measure from the enviroment\n",
    "    - In **GridWorld** , that would be your position on the board\n",
    "    - In **Tic-tac-toe**, the specific configuration pf pieces on the board\n",
    "    - In Video games, like Atari, pixels on the screen or perhaps other inputs such as, the number of lives you have left\n",
    "\n",
    "###### *Actions*\n",
    "- Anything the agent can do while in a state\n",
    "- **Tic-tac-toe**: Placing a piece somewhere on the board\n",
    "-  **Video game**: Moving (up/down/left/right), pressing an action button\n",
    "\n",
    "###### *Rewards*\n",
    "- *Agent receives a reward at every time step*\n",
    "- Rewards are real-valued\n",
    "- Goal of agent is to maximize total future reward\n",
    "- Careful to define rewards the rightway <span style='color:blue'> (We have to be careful how we define rewards)</span>\n",
    "\n",
    "    - **Example:** Robot trying to solve a maze receives a reward of 0 at every step and 1 for solving the maze\n",
    "    - Possible that robot will never solve the maze, or solve it very inefficiently\n",
    "    - Why?\n",
    "    - It has only experienced 0 rewards, thinks that is the best it can do, no incentive to not act randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:blue'>  Another possibility is that it might learn to solve the maze but only through random movements since it has no incentive to solve the maze as quickly as possible.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:blue'> A better solution would be to assign a reward of minus one for every step in this way rewards are always negative. Remember these are numbers on a real-valued scale. The agent doesn't assign any connotation to positive or negative like we do. A negative reward is a penalty.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Better solution is $-1$ reward at every time step\n",
    "- This is a penalty as an incentive to solve the maze as quickly as possible.\n",
    "- Just a number on a scale\n",
    "    - Example: -3 is a better reward than -300\n",
    "    - Later we find that OpenAI Gym enviroments assign us rewards\n",
    "    - Been debated whether or not we should override the default rewards\n",
    "    - In real world enviroments, we would be the ones defining rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:blue'>\n",
    "You'll see later that opening game environments actually assign us the reward. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:blue'> It's been debated whether or not it's a good idea to assign rewards other than the default reward that's provided.</span>\n",
    "\n",
    "<span style='color:blue'> In real life we would be assigning the rewards because we have to. There is no one else that will assign rewards to our agents. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### *Rewards*\n",
    "- At first glance, might seem unnecessary\n",
    "$$p(s',r | s,)$$\n",
    "- If I do **action $a$**, while in **state $s$**, won't always go to $s'$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:blue'>  Next we have state transition probabilities. At first glance these might seem unnecessary. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Gridworld:** action \"up\", should always take you to the square above\n",
    "- **Some enviroments are stochastic.** (Not all enviroments are deterministic - have a source of randomness )\n",
    "- Many only reflect partial knowledge\n",
    "- Breakout: is the ball is moving up or down?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:blue'>  Consider grid world, If you press the button you should go up to the next square. This square should always yield the same reward. This seems deterministic.\n",
    "\n",
    "So you might wonder why we need \n",
    "to model this as a probability.\n",
    "\n",
    "The answer is that not all environments are deterministic so the environment itself can be one source\n",
    "\n",
    "of randomness.\n",
    "\n",
    "Another source of randomness is that our reading of the state can be imperfect.\n",
    "\n",
    "It only reflects partial knowledge of the environment. \n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use an example we've seen before.\n",
    "\n",
    "Consider this one frame of the game Breakout from this frame alone.\n",
    "\n",
    "We can't tell if the ball is moving up or down.\n",
    "\n",
    "Therefore the next state is not deterministic.\n",
    "\n",
    "This is also where Markov comes from in the term Markov decision process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual when we say Markov we usually mean first order Markov which means that we model the next state\n",
    "\n",
    "and reward probability as being dependent only on the previous state an action rather than all previous\n",
    "\n",
    "states in all previous actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
